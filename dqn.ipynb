{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from collections import deque, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# rewards\n",
    "WIN = 10\n",
    "LOSS = -10 \n",
    "DRAW = 5\n",
    "ONGOING = 0\n",
    "INVALID_MOVE = -5\n",
    "\n",
    "\n",
    "\n",
    "PLAYER = 1\n",
    "OPPONENT = -1\n",
    "EMPTY = 0\n",
    "SYMBOLS = {PLAYER: 'X', OPPONENT: 'O', EMPTY: '.'}\n",
    "\n",
    "BOARD_SIZE = 3\n",
    "\n",
    "\n",
    "\n",
    "MAX_MEMORY = 10_000\n",
    "BATCH_SIZE = 100\n",
    "LR = 0.001  # learning rate\n",
    "N_EPISODES = 1000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = N_EPISODES/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe():\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros(shape=(BOARD_SIZE,BOARD_SIZE), dtype=int)\n",
    "        self.current_player = PLAYER  # \"X\" -> 1 (player), \"O\" -> -1 (bot), \"Empty\" -> 0\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros(shape=(BOARD_SIZE,BOARD_SIZE), dtype=int)\n",
    "        self.current_player = PLAYER\n",
    "\n",
    "\n",
    "    def make_move(self, row: int, col: int):\n",
    "        self.board[row, col] = self.current_player\n",
    "        self.current_player *= -1  # switch player\n",
    "    \n",
    "\n",
    "    def find_available_moves(self) -> list:\n",
    "        return list(zip(*np.where(self.board == EMPTY)))\n",
    "\n",
    "\n",
    "    def print_board(self):\n",
    "        \"\"\"Print the Tic-Tac-Toe board.\"\"\"\n",
    "        for row in self.board:\n",
    "            print(\" \".join(SYMBOLS[cell] for cell in row))\n",
    "        print()\n",
    "\n",
    "\n",
    "    def check_winner(self):\n",
    "        for p in [PLAYER, OPPONENT]:\n",
    "            for i in range(BOARD_SIZE):\n",
    "                # Check rows and cols\n",
    "                if np.all(self.board[i, :] == p) or np.all(self.board[:, i] == p):\n",
    "                    return p\n",
    "            # Check diagonals\n",
    "            if np.all(np.diag(self.board) == p) or np.all(np.diag(np.fliplr(self.board)) == p):\n",
    "                return p\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=256, output_size=9):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def train_step(self, states, actions, rewards, next_states, is_terminals):\n",
    "        # Convert to tensors with proper batch dimensions\n",
    "        states = torch.tensor(np.array(states).reshape(-1, 9)).float()\n",
    "        next_states = torch.tensor(np.array(next_states).reshape(-1, 9)).float()\n",
    "        actions = torch.tensor(actions).long().view(-1, 1)\n",
    "        rewards = torch.tensor(rewards).float().view(-1, 1)\n",
    "        is_terminals = torch.tensor(is_terminals).bool()\n",
    "\n",
    "        # 1. Get current Q values for chosen actions\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "\n",
    "        # 2. Calculate target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.model(next_states).max(1)[0].view(-1,1)\n",
    "            target_q = rewards + (self.gamma * next_q * ~is_terminals)  # ~is_terminals prevets terminal states from being included into possible future rewards\n",
    "\n",
    "        # 3. Compute loss\n",
    "        loss = self.criterion(current_q, target_q)\n",
    "\n",
    "        # 4. Backpropagate\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from tic_tac_toe_model.pth\n",
      "The game starts now! Type 'q q' to quit...\n",
      ". . .\n",
      ". . .\n",
      ". . X\n",
      "\n",
      ". . .\n",
      ". O .\n",
      ". X X\n",
      "\n",
      "O . .\n",
      "X O .\n",
      ". X X\n",
      "\n",
      "O X .\n",
      "X O .\n",
      "O X X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0\n",
    "        self.gamma = 0.9\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.model = Linear_QNet(input_size=9, hidden_size=256, output_size=9)\n",
    "        self.trainer = QTrainer(self.model, LR, self.gamma)\n",
    "        self.model_name = \"tic_tac_toe_model.pth\"\n",
    "\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), self.model_name)\n",
    "        print(f\"Model saved to {self.model_name}\")\n",
    "        \n",
    "\n",
    "    def load_model(self):\n",
    "        if os.path.exists(self.model_name):\n",
    "            self.model.load_state_dict(torch.load(self.model_name))\n",
    "            self.model.eval()\n",
    "            print(f\"Loaded model from {self.model_name}\")\n",
    "        else:\n",
    "            print(f\"No saved model found at {self.model_name}\")\n",
    "\n",
    "\n",
    "    def get_state(self, game: TicTacToe) -> list:\n",
    "        state = game.board.copy()\n",
    "        return state\n",
    "    \n",
    "\n",
    "    def get_action(self, state) -> tuple:\n",
    "        # epsilon-greedy strategy\n",
    "        self.epsilon = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * self.n_games / EPS_DECAY)  # exponential decay\n",
    "\n",
    "        if random.choice(range(0, 100)) < self.epsilon:\n",
    "            # random move (exploration)\n",
    "            best_move_index = random.randint(0,8)\n",
    "        else:\n",
    "            # use a model to predict (exploitation)\n",
    "            board_tensor = torch.tensor(state.flatten(), dtype=torch.float32)\n",
    "            Q = self.model(board_tensor)\n",
    "            Q = Q.detach().numpy()\n",
    "            best_move_index = np.argmax(Q)\n",
    "\n",
    "        return tuple((best_move_index // 3, best_move_index % 3))\n",
    "    \n",
    "\n",
    "    def remember(self, old_state, action, reward, new_state, is_terminal):\n",
    "        self.memory.append((old_state, action, reward, new_state, is_terminal))\n",
    "\n",
    "\n",
    "    def train_short_memory(self, old_state, action, reward, new_state, is_terminal):\n",
    "        action_index = action[0] * 3 + action[1]\n",
    "        self.remember(old_state, action_index, reward, new_state, is_terminal)\n",
    "        self.trainer.train_step(\n",
    "            [old_state],\n",
    "            [action_index],\n",
    "            [reward],\n",
    "            [new_state],\n",
    "            [is_terminal])\n",
    "\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            sample = random.sample(self.memory, BATCH_SIZE)\n",
    "        else:\n",
    "            sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, is_terminals = zip(*sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, is_terminals)\n",
    "\n",
    "\n",
    "\n",
    "def random_player(available_moves):\n",
    "    return random.choice(available_moves)\n",
    "\n",
    "\n",
    "\n",
    "def train_ai():\n",
    "    game = TicTacToe()\n",
    "    agent = Agent()\n",
    "    AI = PLAYER\n",
    "    RANDOM = OPPONENT\n",
    "\n",
    "    ai_wins = 0\n",
    "    random_wins = 0\n",
    "    draws = 0\n",
    "\n",
    "    game_logs = np.zeros(N_EPISODES)\n",
    "\n",
    "    for episode in range(N_EPISODES):\n",
    "        game.reset()\n",
    "        while True:\n",
    "            # AI Turn\n",
    "            old_state = agent.get_state(game)\n",
    "\n",
    "            # Valid move check\n",
    "            while True:\n",
    "                action = agent.get_action(old_state)\n",
    "                row, col = action\n",
    "            \n",
    "                # Check move validity\n",
    "                if (row, col) in game.find_available_moves():\n",
    "                    # Valid move\n",
    "                    game.make_move(row, col)\n",
    "                    new_state = agent.get_state(game)\n",
    "                    break\n",
    "                # else:\n",
    "                    # # Invalid move punishment\n",
    "                    # agent.train_short_memory(old_state, action, INVALID_MOVE, old_state, False)\n",
    "            \n",
    "            # AI wins - end episode\n",
    "            if game.check_winner() == AI:\n",
    "                # agent.train_short_memory(old_state, action, WIN, new_state, True)\n",
    "                ai_wins+=1\n",
    "                game_logs[episode] = 1\n",
    "                break\n",
    "\n",
    "            # Draw - end episode (after AI's episode)\n",
    "            if len(game.find_available_moves()) == 0:\n",
    "                # agent.train_short_memory(old_state, action, DRAW, new_state, True)\n",
    "                draws+=1\n",
    "                game_logs[episode] = 0\n",
    "                break\n",
    "\n",
    "            r_row, r_col = random_player(game.find_available_moves())\n",
    "            game.make_move(r_row, r_col)\n",
    "            updated_state = agent.get_state(game)\n",
    "            # Random player wins - end episode\n",
    "            if game.check_winner() == RANDOM:\n",
    "                # agent.train_short_memory(old_state, action, LOSS, new_state, True)\n",
    "                random_wins += 1\n",
    "                game_logs[episode] = -1\n",
    "                break\n",
    "\n",
    "            # Draw - end episode (after player's move)\n",
    "            if len(game.find_available_moves()) == 0:\n",
    "                # agent.train_short_memory(old_state, action, DRAW, new_state, True)\n",
    "                draws+=1\n",
    "                game_logs[episode] = 0\n",
    "                break\n",
    "\n",
    "            # agent.train_short_memory(old_state, action, ONGOING, new_state, False)\n",
    "\n",
    "        agent.n_games+=1\n",
    "        # agent.train_long_memory()\n",
    "    \n",
    "    print(\"AI wins: \", ai_wins)\n",
    "    print(\"Random_player wins: \", random_wins)\n",
    "    print(\"Draws: \", draws)\n",
    "\n",
    "    # Count alltogether\n",
    "    win_logs = []\n",
    "    draw_logs = []\n",
    "    loss_logs = []\n",
    "    for i in range(50, len(game_logs)+1, 50):\n",
    "        counter = Counter(game_logs[:i])\n",
    "        win_logs.append(counter[1])\n",
    "        draw_logs.append(counter[0])\n",
    "        loss_logs.append(counter[-1])\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(0, len(game_logs), 50), win_logs, label=\"Wins\")\n",
    "    plt.plot(range(0, len(game_logs), 50), draw_logs, label=\"Draws\")\n",
    "    plt.plot(range(0, len(game_logs), 50), loss_logs, label=\"Losses\")\n",
    "    plt.title(\"AI Learning Progress Over Time\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Result\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Count per 50 samples\n",
    "    win_logs = []\n",
    "    draw_logs = []\n",
    "    loss_logs = []\n",
    "    for i in range(0, len(game_logs), 50):\n",
    "        counter = Counter(game_logs[i:i+50])\n",
    "        win_logs.append(counter[1])\n",
    "        draw_logs.append(counter[0])\n",
    "        loss_logs.append(counter[-1])\n",
    "\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(0, len(game_logs), 50), win_logs, label=\"Wins\")\n",
    "    plt.plot(range(0, len(game_logs), 50), draw_logs, label=\"Draws\")\n",
    "    plt.plot(range(0, len(game_logs), 50), loss_logs, label=\"Losses\")\n",
    "    plt.title(\"AI Learning Progress Over Time\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Result\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    agent.save_model()\n",
    "    return agent\n",
    "\n",
    "\n",
    "def ai_starts(game: TicTacToe, ai: Agent):\n",
    "    print(\"The game starts now! Type 'q q' to quit...\")\n",
    "    while True:\n",
    "        # AI's move\n",
    "        state = ai.get_state(game)\n",
    "        # Valid move check\n",
    "        while True:\n",
    "            action = ai.get_action(state)\n",
    "            row, col = action\n",
    "        \n",
    "            available_moves = game.find_available_moves()\n",
    "            # Check move validity\n",
    "            if (row, col) in available_moves:\n",
    "                # Valid move\n",
    "                game.make_move(row, col)\n",
    "                break\n",
    "            elif len(available_moves) == 0:\n",
    "                # no moves left - terminal state\n",
    "                break\n",
    "\n",
    "        if len(game.find_available_moves()) == 0:\n",
    "            print(\"It's a draw!\")\n",
    "            game.print_board()\n",
    "            game.reset()\n",
    "            continue\n",
    "        \n",
    "        # Player's move\n",
    "        game.print_board()\n",
    "        row, col = input(\"Make a move!\").split()\n",
    "        if row == 'q' or col == 'q':\n",
    "            break\n",
    "        game.make_move(int(row), int(col))\n",
    "\n",
    "        if game.check_winner() is not None:\n",
    "            print(\"We have a winner!\")\n",
    "            game.print_board()\n",
    "            game.reset()\n",
    "        elif len(game.find_available_moves()) == 0:\n",
    "            print(\"It's a draw!\")\n",
    "            game.print_board()\n",
    "            game.reset()\n",
    "\n",
    "\n",
    "def player_starts(game: TicTacToe, ai: Agent):\n",
    "    print(\"The game starts now! Type 'q q' to quit...\")\n",
    "    while True:\n",
    "        # Player's move\n",
    "        game.print_board()\n",
    "        row, col = input(\"Make a move!\").split()\n",
    "        if row == 'q' or col == 'q':\n",
    "            break\n",
    "        game.make_move(int(row), int(col))\n",
    "\n",
    "        # AI's move\n",
    "        state = ai.get_state(game)\n",
    "        # Valid move check\n",
    "        while True:\n",
    "            action = ai.get_action(state)\n",
    "            row, col = action\n",
    "        \n",
    "            available_moves = game.find_available_moves()\n",
    "            # Check move validity\n",
    "            if (row, col) in available_moves:\n",
    "                # Valid move\n",
    "                game.make_move(row, col)\n",
    "                break\n",
    "            elif len(available_moves) == 0:\n",
    "                # no moves left - terminal state\n",
    "                break\n",
    "\n",
    "        if game.check_winner() is not None:\n",
    "            print(\"We have a winner!\")\n",
    "            game.print_board()\n",
    "            game.reset()\n",
    "        elif len(game.find_available_moves()) == 0:\n",
    "            print(\"It's a draw!\")\n",
    "            game.print_board()\n",
    "            game.reset()\n",
    "\n",
    "\n",
    "def play(if_player_starts):\n",
    "    game = TicTacToe()\n",
    "    ai = Agent()\n",
    "    ai.load_model()\n",
    "    if not os.path.exists(ai.model_name):\n",
    "        print(\"No trained model found, training new one...\")\n",
    "        ai = train_ai()\n",
    "        ai.save_model()\n",
    "    if if_player_starts.lower() == \"yes\":\n",
    "        player_starts(game, ai)\n",
    "    elif if_player_starts.lower() == \"no\":\n",
    "        ai_starts(game, ai)\n",
    "    else:\n",
    "        print(\"Invalid input, bye!\")\n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    play(input(\"Do you want to start? (yes/no): \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
