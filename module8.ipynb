{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training game: 0\n",
      "Training game: 1\n",
      "Training game: 2\n",
      "Training game: 3\n",
      "Training game: 4\n",
      "Training game: 5\n",
      "Training game: 6\n",
      "Training game: 7\n",
      "Training game: 8\n",
      "Training game: 9\n",
      "Training game: 10\n",
      "Training game: 11\n",
      "Training game: 12\n",
      "Training game: 13\n",
      "Training game: 14\n",
      "Training game: 15\n",
      "Training game: 16\n",
      "Training game: 17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 187\u001b[0m\n\u001b[0;32m    184\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy_network\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Train the policy network\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m train_policy_network(policy_network, optimizer, games\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, board_size\u001b[38;5;241m=\u001b[39mboard_size)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Test the trained network\u001b[39;00m\n\u001b[0;32m    190\u001b[0m winners \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[1], line 146\u001b[0m, in \u001b[0;36mtrain_policy_network\u001b[1;34m(policy_network, optimizer, games, board_size)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m game\u001b[38;5;241m.\u001b[39mis_terminal(): \u001b[38;5;66;03m# play 1 game \u001b[39;00m\n\u001b[0;32m    145\u001b[0m     root \u001b[38;5;241m=\u001b[39m MCTSNode(game, policy_network\u001b[38;5;241m=\u001b[39mpolicy_network)\n\u001b[1;32m--> 146\u001b[0m     move \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39mrun_mcts(\u001b[38;5;241m1000\u001b[39m) \u001b[38;5;66;03m# get best move according to mcts \u001b[39;00m\n\u001b[0;32m    147\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(game\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mflatten()) \u001b[38;5;66;03m# save the states of the game\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     move_index \u001b[38;5;241m=\u001b[39m move[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m board_size \u001b[38;5;241m+\u001b[39m move[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Convert (r, c) to flat index\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 132\u001b[0m, in \u001b[0;36mMCTSNode.run_mcts\u001b[1;34m(self, iterations)\u001b[0m\n\u001b[0;32m    130\u001b[0m     node \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mselect_child()\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39muntried_moves:\n\u001b[1;32m--> 132\u001b[0m     node \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mexpand()\n\u001b[0;32m    133\u001b[0m reward \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39msimulate()\n\u001b[0;32m    134\u001b[0m node\u001b[38;5;241m.\u001b[39mbackpropagate(reward)\n",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m, in \u001b[0;36mMCTSNode.expand\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    106\u001b[0m new_state\u001b[38;5;241m.\u001b[39mmake_move(move)\n\u001b[1;32m--> 107\u001b[0m child \u001b[38;5;241m=\u001b[39m MCTSNode(new_state, parent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, move\u001b[38;5;241m=\u001b[39mmove, policy_network\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend(child)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m child\n",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m, in \u001b[0;36mMCTSNode.__init__\u001b[1;34m(self, state, parent, move, policy_network)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muntried_moves \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mget_available_moves()\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network \u001b[38;5;241m=\u001b[39m policy_network\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prior_probabilities()\n",
      "Cell \u001b[1;32mIn[1], line 87\u001b[0m, in \u001b[0;36mMCTSNode.get_prior_probabilities\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {move: \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m move \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muntried_moves}\n\u001b[1;32m---> 87\u001b[0m board_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mflatten())\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     89\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_network(board_tensor)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, board_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        self.input_size = board_size * board_size\n",
    "        self.fc1 = nn.Linear(self.input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, self.input_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self, board_size=3):\n",
    "        self.board_size = board_size\n",
    "        self.board = np.zeros((board_size, board_size), dtype=int)  # 0 = empty, 1 = X, -1 = O\n",
    "        self.current_player = 1  # X starts\n",
    "\n",
    "    def get_available_moves(self):\n",
    "        return [(r, c) for r in range(self.board_size) for c in range(self.board_size) if self.board[r, c] == 0]\n",
    "\n",
    "    def make_move(self, move):\n",
    "        r, c = move\n",
    "        self.board[r, c] = self.current_player\n",
    "        self.current_player *= -1  # Switch players\n",
    "\n",
    "    def undo_move(self, move):\n",
    "        r, c = move\n",
    "        self.board[r, c] = 0\n",
    "        self.current_player *= -1\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows and columns\n",
    "        for i in range(self.board_size):\n",
    "            if np.all(self.board[i, :] == 1) or np.all(self.board[:, i] == 1):\n",
    "                return 1  # X wins\n",
    "            if np.all(self.board[i, :] == -1) or np.all(self.board[:, i] == -1):\n",
    "                return -1  # O wins\n",
    "\n",
    "        # Check diagonals\n",
    "        if np.all(self.board.diagonal() == 1) or np.all(np.fliplr(self.board).diagonal() == 1):\n",
    "            return 1  # X wins\n",
    "        if np.all(self.board.diagonal() == -1) or np.all(np.fliplr(self.board).diagonal() == -1):\n",
    "            return -1  # O wins\n",
    "\n",
    "        # Check for draw\n",
    "        if not self.get_available_moves():\n",
    "            return 0  # Draw\n",
    "\n",
    "        return None  # Game not over\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.check_winner() is not None\n",
    "\n",
    "    def clone(self):\n",
    "        new_game = TicTacToe(self.board_size)\n",
    "        new_game.board = self.board.copy()\n",
    "        new_game.current_player = self.current_player\n",
    "        return new_game\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, move=None, policy_network=None):\n",
    "        self.state = state.clone()\n",
    "        self.parent = parent\n",
    "        self.move = move\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0  \n",
    "        self.untried_moves = state.get_available_moves()\n",
    "        self.policy_network = policy_network\n",
    "        self.prior_probabilities = self.get_prior_probabilities()\n",
    "\n",
    "    def get_prior_probabilities(self):\n",
    "        if self.policy_network is None:\n",
    "            return {move: 1.0 for move in self.untried_moves}\n",
    "        board_tensor = torch.FloatTensor(self.state.board.flatten()).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            probabilities = self.policy_network(board_tensor).squeeze().numpy()\n",
    "        return {(r, c): probabilities[r * self.state.board_size + c] for r, c in self.untried_moves}\n",
    "\n",
    "    def select_child(self):\n",
    "        def ucb_score(child):\n",
    "            if child.visits == 0:\n",
    "                return float('inf')\n",
    "            exploitation = child.value / child.visits\n",
    "            exploration = 1.41 * np.sqrt(np.log(self.visits + 1) / child.visits)\n",
    "            prior = self.prior_probabilities[child.move]\n",
    "            return exploitation + exploration * prior\n",
    "\n",
    "        return max(self.children, key=ucb_score)\n",
    "\n",
    "    def expand(self):\n",
    "        move = self.untried_moves.pop()\n",
    "        new_state = self.state.clone()\n",
    "        new_state.make_move(move)\n",
    "        child = MCTSNode(new_state, parent=self, move=move, policy_network=self.policy_network)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "\n",
    "    def backpropagate(self, reward):\n",
    "        self.visits += 1\n",
    "        self.value += reward\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(-reward)  \n",
    "\n",
    "    def best_move(self):\n",
    "        return max(self.children, key=lambda c: c.visits).move if self.children else random.choice(self.state.get_available_moves())\n",
    "\n",
    "    def simulate(self):\n",
    "        temp_state = self.state.clone()\n",
    "        while not temp_state.is_terminal():\n",
    "            temp_state.make_move(random.choice(temp_state.get_available_moves()))\n",
    "        return temp_state.check_winner() or 0\n",
    "\n",
    "    def run_mcts(self, iterations=1000):\n",
    "        for _ in range(iterations):\n",
    "            node = self\n",
    "            while node.children and not node.untried_moves:\n",
    "                node = node.select_child()\n",
    "            if node.untried_moves:\n",
    "                node = node.expand()\n",
    "            reward = node.simulate()\n",
    "            node.backpropagate(reward)\n",
    "        return self.best_move()\n",
    "\n",
    "# Training the policy network\n",
    "def train_policy_network(policy_network, optimizer, games, board_size):\n",
    "    for game_num in range(games):\n",
    "        print(\"Training game:\", game_num)\n",
    "        game = TicTacToe(board_size)\n",
    "        states = []\n",
    "        played_moves = []\n",
    "        while not game.is_terminal(): # play 1 game \n",
    "            root = MCTSNode(game, policy_network=policy_network)\n",
    "            move = root.run_mcts(1000) # get best move according to mcts \n",
    "            states.append(game.board.flatten()) # save the states of the game\n",
    "            move_index = move[0] * board_size + move[1]  # Convert (r, c) to flat index\n",
    "            played_moves.append(F.one_hot(torch.tensor(move_index), num_classes=board_size * board_size).float()) #One-hot encoding\n",
    "            game.make_move(move)\n",
    "\n",
    "        winner = game.check_winner()\n",
    "        if winner == 1:\n",
    "            rewards = [1 if i % 2 == 0 else -1 for i in range(len(states))] # each move played gets the reward 1\n",
    "        elif winner == -1:\n",
    "            rewards = [-1 if i % 2 == 0 else 1 for i in range(len(states))] # each move played gets the reward -1\n",
    "        else:\n",
    "            rewards = [0 for _ in range(len(states))]\n",
    "\n",
    "        optimizer.zero_grad() # Resets the gradients of the policy network’s parameters to zero, otherwise they add up.\n",
    "        for state, move_prob, reward in zip(states, played_moves, rewards):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            predicted_probs = policy_network.forward(state_tensor) # Forward pass: outputs a probability distribution over the 9 possible moves, current \"belief\" about the best moves for the given state.\n",
    "            loss = -torch.sum(torch.FloatTensor(move_prob) * torch.log(predicted_probs)) * reward # Computes the policy gradient loss. Are the moves with a high assigned probability leading to victories?\n",
    "            loss.backward() # Backpropagation \n",
    "        optimizer.step() # Updates the policy network’s parameters using the computed gradients from the backpropagation. \n",
    "\n",
    "# Simulating against a random opponent\n",
    "def simulateGame(board_size, policy_network):\n",
    "    game = TicTacToe(board_size)\n",
    "    while not game.is_terminal():\n",
    "        if game.current_player == 1:\n",
    "            root = MCTSNode(game, policy_network=policy_network)\n",
    "            move = root.run_mcts(1000)\n",
    "        else:\n",
    "            move = random.choice(game.get_available_moves())  # Random opponent\n",
    "        game.make_move(move)\n",
    "\n",
    "    return game.check_winner()\n",
    "\n",
    "# Main execution\n",
    "board_size = 4  # Change this to any board size (e.g., 4 for 4x4)\n",
    "policy_network = PolicyNetwork(board_size)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=0.001)\n",
    "\n",
    "# Train the policy network\n",
    "train_policy_network(policy_network, optimizer, games=100, board_size=board_size)\n",
    "\n",
    "# Test the trained network\n",
    "winners = []\n",
    "for i in range(1, 101):\n",
    "    print(\"Playing game:\", i)\n",
    "    winners.append(simulateGame(board_size, policy_network))\n",
    "\n",
    "print(\"Times AI has won: \", winners.count(1))\n",
    "print(\"Times opponent has won: \", winners.count(-1))\n",
    "print(\"Times a draw happened:\", winners.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training game: 0\n",
      "Training game: 1\n",
      "Training game: 2\n",
      "Training game: 3\n",
      "Training game: 4\n",
      "Training game: 5\n",
      "Training game: 6\n",
      "Training game: 7\n",
      "Training game: 8\n",
      "Training game: 9\n",
      "Training game: 10\n",
      "Training game: 11\n",
      "Training game: 12\n",
      "Training game: 13\n",
      "Training game: 14\n",
      "Training game: 15\n",
      "Training game: 16\n",
      "Training game: 17\n",
      "Training game: 18\n",
      "Training game: 19\n",
      "Training game: 20\n",
      "Training game: 21\n",
      "Training game: 22\n",
      "Training game: 23\n",
      "Training game: 24\n",
      "Training game: 25\n",
      "Training game: 26\n",
      "Training game: 27\n",
      "Training game: 28\n",
      "Training game: 29\n",
      "Training game: 30\n",
      "Training game: 31\n",
      "Training game: 32\n",
      "Training game: 33\n",
      "Training game: 34\n",
      "Training game: 35\n",
      "Training game: 36\n",
      "Training game: 37\n",
      "Training game: 38\n",
      "Training game: 39\n",
      "Training game: 40\n",
      "Training game: 41\n",
      "Training game: 42\n",
      "Training game: 43\n",
      "Training game: 44\n",
      "Training game: 45\n",
      "Training game: 46\n",
      "Training game: 47\n",
      "Training game: 48\n",
      "Training game: 49\n",
      "Training game: 50\n",
      "Training game: 51\n",
      "Training game: 52\n",
      "Training game: 53\n",
      "Training game: 54\n",
      "Training game: 55\n",
      "Training game: 56\n",
      "Training game: 57\n",
      "Training game: 58\n",
      "Training game: 59\n",
      "Training game: 60\n",
      "Training game: 61\n",
      "Training game: 62\n",
      "Training game: 63\n",
      "Training game: 64\n",
      "Training game: 65\n",
      "Training game: 66\n",
      "Training game: 67\n",
      "Training game: 68\n",
      "Training game: 69\n",
      "Training game: 70\n",
      "Training game: 71\n",
      "Training game: 72\n",
      "Training game: 73\n",
      "Training game: 74\n",
      "Training game: 75\n",
      "Training game: 76\n",
      "Training game: 77\n",
      "Training game: 78\n",
      "Training game: 79\n",
      "Training game: 80\n",
      "Training game: 81\n",
      "Training game: 82\n",
      "Training game: 83\n",
      "Training game: 84\n",
      "Training game: 85\n",
      "Training game: 86\n",
      "Training game: 87\n",
      "Training game: 88\n",
      "Training game: 89\n",
      "Training game: 90\n",
      "Training game: 91\n",
      "Training game: 92\n",
      "Training game: 93\n",
      "Training game: 94\n",
      "Training game: 95\n",
      "Training game: 96\n",
      "Training game: 97\n",
      "Training game: 98\n",
      "Training game: 99\n",
      "Playing game: 1\n",
      "Playing game: 2\n",
      "Playing game: 3\n",
      "Playing game: 4\n",
      "Playing game: 5\n",
      "Playing game: 6\n",
      "Playing game: 7\n",
      "Playing game: 8\n",
      "Playing game: 9\n",
      "Playing game: 10\n",
      "Playing game: 11\n",
      "Playing game: 12\n",
      "Playing game: 13\n",
      "Playing game: 14\n",
      "Playing game: 15\n",
      "Playing game: 16\n",
      "Playing game: 17\n",
      "Playing game: 18\n",
      "Playing game: 19\n",
      "Playing game: 20\n",
      "Playing game: 21\n",
      "Playing game: 22\n",
      "Playing game: 23\n",
      "Playing game: 24\n",
      "Playing game: 25\n",
      "Playing game: 26\n",
      "Playing game: 27\n",
      "Playing game: 28\n",
      "Playing game: 29\n",
      "Playing game: 30\n",
      "Playing game: 31\n",
      "Playing game: 32\n",
      "Playing game: 33\n",
      "Playing game: 34\n",
      "Playing game: 35\n",
      "Playing game: 36\n",
      "Playing game: 37\n",
      "Playing game: 38\n",
      "Playing game: 39\n",
      "Playing game: 40\n",
      "Playing game: 41\n",
      "Playing game: 42\n",
      "Playing game: 43\n",
      "Playing game: 44\n",
      "Playing game: 45\n",
      "Playing game: 46\n",
      "Playing game: 47\n",
      "Playing game: 48\n",
      "Playing game: 49\n",
      "Playing game: 50\n",
      "Playing game: 51\n",
      "Playing game: 52\n",
      "Playing game: 53\n",
      "Playing game: 54\n",
      "Playing game: 55\n",
      "Playing game: 56\n",
      "Playing game: 57\n",
      "Playing game: 58\n",
      "Playing game: 59\n",
      "Playing game: 60\n",
      "Playing game: 61\n",
      "Playing game: 62\n",
      "Playing game: 63\n",
      "Playing game: 64\n",
      "Playing game: 65\n",
      "Playing game: 66\n",
      "Playing game: 67\n",
      "Playing game: 68\n",
      "Playing game: 69\n",
      "Playing game: 70\n",
      "Playing game: 71\n",
      "Playing game: 72\n",
      "Playing game: 73\n",
      "Playing game: 74\n",
      "Playing game: 75\n",
      "Playing game: 76\n",
      "Playing game: 77\n",
      "Playing game: 78\n",
      "Playing game: 79\n",
      "Playing game: 80\n",
      "Playing game: 81\n",
      "Playing game: 82\n",
      "Playing game: 83\n",
      "Playing game: 84\n",
      "Playing game: 85\n",
      "Playing game: 86\n",
      "Playing game: 87\n",
      "Playing game: 88\n",
      "Playing game: 89\n",
      "Playing game: 90\n",
      "Playing game: 91\n",
      "Playing game: 92\n",
      "Playing game: 93\n",
      "Playing game: 94\n",
      "Playing game: 95\n",
      "Playing game: 96\n",
      "Playing game: 97\n",
      "Playing game: 98\n",
      "Playing game: 99\n",
      "Playing game: 100\n",
      "Times AI has won:  10\n",
      "Times opponent has won:  14\n",
      "Times a draw happened: 76\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvPolicyNetwork(nn.Module):\n",
    "    def __init__(self, board_size):\n",
    "        super(ConvPolicyNetwork, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * board_size * board_size, 128)\n",
    "        self.fc2 = nn.Linear(128, board_size * board_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, self.board_size, self.board_size)  # Reshape to (batch_size, 1, board_size, board_size)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32 * self.board_size * self.board_size)  # Flatten for fully connected layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self, board_size=3):\n",
    "        self.board_size = board_size\n",
    "        self.board = np.zeros((board_size, board_size), dtype=int)  # 0 = empty, 1 = X, -1 = O\n",
    "        self.current_player = 1  # X starts\n",
    "\n",
    "    def get_available_moves(self):\n",
    "        return [(r, c) for r in range(self.board_size) for c in range(self.board_size) if self.board[r, c] == 0]\n",
    "\n",
    "    def make_move(self, move):\n",
    "        r, c = move\n",
    "        self.board[r, c] = self.current_player\n",
    "        self.current_player *= -1  # Switch players\n",
    "\n",
    "    def undo_move(self, move):\n",
    "        r, c = move\n",
    "        self.board[r, c] = 0\n",
    "        self.current_player *= -1\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check rows and columns\n",
    "        for i in range(self.board_size):\n",
    "            if np.all(self.board[i, :] == 1) or np.all(self.board[:, i] == 1):\n",
    "                return 1  # X wins\n",
    "            if np.all(self.board[i, :] == -1) or np.all(self.board[:, i] == -1):\n",
    "                return -1  # O wins\n",
    "\n",
    "        # Check diagonals\n",
    "        if np.all(self.board.diagonal() == 1) or np.all(np.fliplr(self.board).diagonal() == 1):\n",
    "            return 1  # X wins\n",
    "        if np.all(self.board.diagonal() == -1) or np.all(np.fliplr(self.board).diagonal() == -1):\n",
    "            return -1  # O wins\n",
    "\n",
    "        # Check for draw\n",
    "        if not self.get_available_moves():\n",
    "            return 0  # Draw\n",
    "\n",
    "        return None  # Game not over\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.check_winner() is not None\n",
    "\n",
    "    def clone(self):\n",
    "        new_game = TicTacToe(self.board_size)\n",
    "        new_game.board = self.board.copy()\n",
    "        new_game.current_player = self.current_player\n",
    "        return new_game\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, move=None, policy_network=None):\n",
    "        self.state = state.clone()\n",
    "        self.parent = parent\n",
    "        self.move = move\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0  \n",
    "        self.untried_moves = state.get_available_moves()\n",
    "        self.policy_network = policy_network\n",
    "        self.prior_probabilities = self.get_prior_probabilities()\n",
    "\n",
    "    def get_prior_probabilities(self):\n",
    "        if self.policy_network is None:\n",
    "            return {move: 1.0 for move in self.untried_moves}\n",
    "        board_tensor = torch.FloatTensor(self.state.board.flatten()).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            probabilities = self.policy_network(board_tensor).squeeze().numpy()\n",
    "        return {(r, c): probabilities[r * self.state.board_size + c] for r, c in self.untried_moves}\n",
    "\n",
    "    def select_child(self):\n",
    "        def ucb_score(child):\n",
    "            if child.visits == 0:\n",
    "                return float('inf')\n",
    "            exploitation = child.value / child.visits\n",
    "            exploration = 1.41 * np.sqrt(np.log(self.visits + 1) / child.visits)\n",
    "            prior = self.prior_probabilities[child.move]\n",
    "            return exploitation + exploration * prior\n",
    "\n",
    "        return max(self.children, key=ucb_score)\n",
    "\n",
    "    def expand(self):\n",
    "        move = self.untried_moves.pop()\n",
    "        new_state = self.state.clone()\n",
    "        new_state.make_move(move)\n",
    "        child = MCTSNode(new_state, parent=self, move=move, policy_network=self.policy_network)\n",
    "        self.children.append(child)\n",
    "        return child\n",
    "\n",
    "    def backpropagate(self, reward):\n",
    "        self.visits += 1\n",
    "        self.value += reward\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(-reward)  \n",
    "\n",
    "    def best_move(self):\n",
    "        return max(self.children, key=lambda c: c.visits).move if self.children else random.choice(self.state.get_available_moves())\n",
    "\n",
    "    def simulate(self):\n",
    "        temp_state = self.state.clone()\n",
    "        while not temp_state.is_terminal():\n",
    "            temp_state.make_move(random.choice(temp_state.get_available_moves()))\n",
    "        return temp_state.check_winner() or 0\n",
    "\n",
    "    def run_mcts(self, iterations=1000):\n",
    "        for _ in range(iterations):\n",
    "            node = self\n",
    "            while node.children and not node.untried_moves:\n",
    "                node = node.select_child()\n",
    "            if node.untried_moves:\n",
    "                node = node.expand()\n",
    "            reward = node.simulate()\n",
    "            node.backpropagate(reward)\n",
    "        return self.best_move()\n",
    "\n",
    "# Training the policy network\n",
    "def train_policy_network(policy_network, optimizer, games, board_size):\n",
    "    for game_num in range(games):\n",
    "        print(\"Training game:\", game_num)\n",
    "        game = TicTacToe(board_size)\n",
    "        states = []\n",
    "        played_moves = []\n",
    "        while not game.is_terminal(): # play 1 game \n",
    "            root = MCTSNode(game, policy_network=policy_network)\n",
    "            move = root.run_mcts(1000) # get best move according to mcts \n",
    "            states.append(game.board.flatten()) # save the states of the game\n",
    "            move_index = move[0] * board_size + move[1]  # Convert (r, c) to flat index\n",
    "            played_moves.append(F.one_hot(torch.tensor(move_index), num_classes=board_size * board_size).float()) #One-hot encoding\n",
    "            game.make_move(move)\n",
    "\n",
    "        winner = game.check_winner()\n",
    "        if winner == 1:\n",
    "            rewards = [1 if i % 2 == 0 else -1 for i in range(len(states))] # each move played gets the reward 1\n",
    "        elif winner == -1:\n",
    "            rewards = [-1 if i % 2 == 0 else 1 for i in range(len(states))] # each move played gets the reward -1\n",
    "        else:\n",
    "            rewards = [0 for _ in range(len(states))]\n",
    "\n",
    "        optimizer.zero_grad() # Resets the gradients of the policy network’s parameters to zero, otherwise they add up.\n",
    "        for state, move_prob, reward in zip(states, played_moves, rewards):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            predicted_probs = policy_network.forward(state_tensor) # Forward pass: outputs a probability distribution over the 9 possible moves, current \"belief\" about the best moves for the given state.\n",
    "            loss = -torch.sum(torch.FloatTensor(move_prob) * torch.log(predicted_probs)) * reward # Computes the policy gradient loss. Are the moves with a high assigned probability leading to victories?\n",
    "            loss.backward() # Backpropagation \n",
    "        optimizer.step() # Updates the policy network’s parameters using the computed gradients from the backpropagation. \n",
    "\n",
    "# Simulating against a random opponent\n",
    "def simulateGame(board_size, policy_network):\n",
    "    game = TicTacToe(board_size)\n",
    "    while not game.is_terminal():\n",
    "        if game.current_player == 1:\n",
    "            root = MCTSNode(game, policy_network=policy_network)\n",
    "            move = root.run_mcts(1000)\n",
    "        else:\n",
    "            move = random.choice(game.get_available_moves())  # Random opponent\n",
    "        game.make_move(move)\n",
    "\n",
    "    return game.check_winner()\n",
    "\n",
    "# Main execution\n",
    "board_size = 4  # Change this to any board size (e.g., 4 for 4x4)\n",
    "policy_network = ConvPolicyNetwork(board_size)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr=0.001)\n",
    "\n",
    "# Train the policy network\n",
    "train_policy_network(policy_network, optimizer, games=100, board_size=board_size)\n",
    "\n",
    "# Test the trained network\n",
    "winners = []\n",
    "for i in range(1, 101):\n",
    "    print(\"Playing game:\", i)\n",
    "    winners.append(simulateGame(board_size, policy_network))\n",
    "\n",
    "print(\"Times AI has won: \", winners.count(1))\n",
    "print(\"Times opponent has won: \", winners.count(-1))\n",
    "print(\"Times a draw happened:\", winners.count(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
